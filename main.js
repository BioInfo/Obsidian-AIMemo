/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __markAsModule = (target) => __defProp(target, "__esModule", { value: true });
var __export = (target, all) => {
  __markAsModule(target);
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __reExport = (target, module2, desc) => {
  if (module2 && typeof module2 === "object" || typeof module2 === "function") {
    for (let key of __getOwnPropNames(module2))
      if (!__hasOwnProp.call(target, key) && key !== "default")
        __defProp(target, key, { get: () => module2[key], enumerable: !(desc = __getOwnPropDesc(module2, key)) || desc.enumerable });
  }
  return target;
};
var __toModule = (module2) => {
  return __reExport(__markAsModule(__defProp(module2 != null ? __create(__getProtoOf(module2)) : {}, "default", module2 && module2.__esModule && "default" in module2 ? { get: () => module2.default, enumerable: true } : { value: module2, enumerable: true })), module2);
};

// src/main.ts
__export(exports, {
  default: () => AiVoiceMemoPlugin
});
var import_obsidian6 = __toModule(require("obsidian"));

// src/managers/voice-recorder-manager.ts
var import_obsidian = __toModule(require("obsidian"));

// src/utils/errors.ts
var PluginError = class extends Error {
  constructor(message, code, originalError) {
    super(message);
    this.code = code;
    this.originalError = originalError;
    this.name = this.constructor.name;
  }
};
var VoiceRecorderError = class extends PluginError {
  constructor(message, code, originalError) {
    super(message, code, originalError);
    this.code = code;
  }
};
var TranscriptionError = class extends PluginError {
  constructor(message, code, originalError) {
    super(message, code, originalError);
    this.code = code;
  }
};
var VoiceRecorderErrorCode;
(function(VoiceRecorderErrorCode2) {
  VoiceRecorderErrorCode2["ALREADY_RECORDING"] = "ALREADY_RECORDING";
  VoiceRecorderErrorCode2["NO_RECORDING_ACTIVE"] = "NO_RECORDING_ACTIVE";
  VoiceRecorderErrorCode2["PERMISSION_DENIED"] = "PERMISSION_DENIED";
  VoiceRecorderErrorCode2["UNSUPPORTED_FORMAT"] = "UNSUPPORTED_FORMAT";
  VoiceRecorderErrorCode2["UNSUPPORTED_BROWSER"] = "UNSUPPORTED_BROWSER";
  VoiceRecorderErrorCode2["DEVICE_NOT_FOUND"] = "DEVICE_NOT_FOUND";
  VoiceRecorderErrorCode2["SAVE_FAILED"] = "SAVE_FAILED";
  VoiceRecorderErrorCode2["UNKNOWN"] = "UNKNOWN";
})(VoiceRecorderErrorCode || (VoiceRecorderErrorCode = {}));
var TranscriptionErrorCode;
(function(TranscriptionErrorCode2) {
  TranscriptionErrorCode2["INVALID_AUDIO"] = "INVALID_AUDIO";
  TranscriptionErrorCode2["INVALID_OPTIONS"] = "INVALID_OPTIONS";
  TranscriptionErrorCode2["API_ERROR"] = "API_ERROR";
  TranscriptionErrorCode2["MODEL_NOT_FOUND"] = "MODEL_NOT_FOUND";
  TranscriptionErrorCode2["QUEUE_FULL"] = "QUEUE_FULL";
  TranscriptionErrorCode2["NOTE_CREATION_FAILED"] = "NOTE_CREATION_FAILED";
  TranscriptionErrorCode2["LOCAL_MODEL_ERROR"] = "LOCAL_MODEL_ERROR";
  TranscriptionErrorCode2["UNKNOWN"] = "UNKNOWN";
})(TranscriptionErrorCode || (TranscriptionErrorCode = {}));
var SummarizationError = class extends PluginError {
  constructor(message, code, originalError) {
    super(message, code, originalError);
    this.code = code;
  }
};
var SummarizationErrorCode;
(function(SummarizationErrorCode2) {
  SummarizationErrorCode2["TEXT_TOO_LONG"] = "TEXT_TOO_LONG";
  SummarizationErrorCode2["PROCESSING_FAILED"] = "PROCESSING_FAILED";
  SummarizationErrorCode2["INVALID_OPTIONS"] = "INVALID_OPTIONS";
  SummarizationErrorCode2["MODEL_ERROR"] = "MODEL_ERROR";
  SummarizationErrorCode2["UNKNOWN"] = "UNKNOWN";
})(SummarizationErrorCode || (SummarizationErrorCode = {}));

// src/managers/voice-recorder-manager.ts
var VoiceRecorderManager = class {
  constructor(plugin, transcriptionService) {
    this.mediaRecorder = null;
    this.audioChunks = [];
    this._isRecording = false;
    this.stream = null;
    this.audioFormat = "audio/webm";
    this.SUPPORTED_FORMATS = [
      "audio/webm",
      "audio/ogg",
      "audio/wav",
      "audio/mp4"
    ];
    this.plugin = plugin;
    this.transcriptionService = transcriptionService;
  }
  isRecording() {
    return this._isRecording;
  }
  async startRecording() {
    if (this.isRecording()) {
      throw new VoiceRecorderError("Already recording!", VoiceRecorderErrorCode.ALREADY_RECORDING);
    }
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      console.error("MediaDevices API not supported");
      throw new VoiceRecorderError("Your system does not support audio recording. Please ensure you are using a supported browser.", VoiceRecorderErrorCode.UNSUPPORTED_BROWSER);
    }
    try {
      console.log("Attempting to access microphone...");
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioDevices = devices.filter((device) => device.kind === "audioinput");
        console.log("Available audio devices:", audioDevices.map((d) => ({ deviceId: d.deviceId, label: d.label })));
        this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log("Microphone access granted successfully");
      } catch (error) {
        if (error instanceof DOMException && error.name === "NotAllowedError") {
          throw new VoiceRecorderError("Microphone access was denied. Please enable microphone access in your system settings:\n1. Open System Settings \u2192 Privacy & Security \u2192 Microphone\n2. Find Obsidian in the list\n3. Toggle the switch next to Obsidian to ON\n4. Restart Obsidian and try again", VoiceRecorderErrorCode.PERMISSION_DENIED, error);
        }
        throw error;
      }
      this.audioFormat = await this.getBestAudioFormat();
      console.log(`Using audio format: ${this.audioFormat}`);
      const options = {
        mimeType: this.audioFormat,
        audioBitsPerSecond: this.getAudioBitrate(this.plugin.settings.audioQuality)
      };
      this.mediaRecorder = new MediaRecorder(this.stream, options);
      this.audioChunks = [];
      this.mediaRecorder.addEventListener("dataavailable", (event) => {
        this.audioChunks.push(event.data);
      });
      this.mediaRecorder.addEventListener("stop", () => {
        this.handleRecordingComplete();
      });
      this.mediaRecorder.start();
      this._isRecording = true;
      new import_obsidian.Notice("Recording started");
    } catch (error) {
      console.error("Error starting recording:", error);
      new import_obsidian.Notice("Failed to start recording. Please check microphone permissions.");
    }
  }
  async stopRecording() {
    var _a;
    if (!this._isRecording || !this.mediaRecorder) {
      throw new VoiceRecorderError("No active recording!", VoiceRecorderErrorCode.NO_RECORDING_ACTIVE);
    }
    this.mediaRecorder.stop();
    (_a = this.stream) == null ? void 0 : _a.getTracks().forEach((track) => track.stop());
    this._isRecording = false;
    new import_obsidian.Notice("Recording stopped");
  }
  async handleRecordingComplete() {
    try {
      const audioBlob = new Blob(this.audioChunks, {
        type: this.audioFormat
      });
      if (this.plugin.settings.saveAudioFiles) {
        await this.saveAudioFile(audioBlob);
      }
      if (this.plugin.settings.autoTranscribe) {
        const jobId = await this.transcriptionService.transcribe(audioBlob);
        new import_obsidian.Notice("Transcription started. Job ID: " + jobId);
      }
    } catch (error) {
      if (error instanceof TranscriptionError) {
        new import_obsidian.Notice(`Transcription error: ${error.message}`);
      } else if (error instanceof VoiceRecorderError) {
        new import_obsidian.Notice(`Recording error: ${error.message}`);
      } else {
        console.error("Unexpected error:", error);
        new import_obsidian.Notice("An unexpected error occurred");
      }
      throw error;
    } finally {
      this.cleanup();
    }
  }
  async saveAudioFile(audioBlob) {
    try {
      const extension = this.audioFormat.split("/")[1].split(";")[0];
      const timestamp = new Date().toISOString().replace(/:/g, "-").replace(/\./g, "-").slice(0, 19);
      const fileName = `voice-memo-${timestamp}.${extension}`;
      const arrayBuffer = await audioBlob.arrayBuffer();
      await this.plugin.app.vault.adapter.writeBinary(`${this.plugin.settings.memoStoragePath}/${fileName}`, arrayBuffer);
    } catch (error) {
      console.error("Error saving audio file:", error);
      throw new VoiceRecorderError("Failed to save audio file", VoiceRecorderErrorCode.SAVE_FAILED, error);
    }
  }
  async getBestAudioFormat() {
    const mimeTypes = [
      "audio/webm",
      "audio/webm;codecs=opus",
      "audio/ogg",
      "audio/ogg;codecs=opus",
      "audio/mp4",
      "audio/wav"
    ];
    const supportedType = mimeTypes.find((type) => MediaRecorder.isTypeSupported(type));
    if (!supportedType) {
      throw new VoiceRecorderError("No supported audio format found. Please try using a different browser.", VoiceRecorderErrorCode.UNSUPPORTED_FORMAT);
    }
    return supportedType;
  }
  getAudioBitrate(quality) {
    switch (quality) {
      case "low":
        return 96e3;
      case "medium":
        return 128e3;
      case "high":
        return 192e3;
      default:
        return 128e3;
    }
  }
  cleanup() {
    if (this._isRecording) {
      this.stopRecording();
    }
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.stream = null;
  }
};

// src/services/transcription-service.ts
var import_obsidian2 = __toModule(require("obsidian"));

// src/services/whisper-api-client.ts
var WhisperAPIClient = class {
  constructor(apiKey) {
    this.baseUrl = "https://api.openai.com/v1/audio/transcriptions";
    this.apiKey = apiKey;
  }
  async transcribe(audioBlob) {
    var _a;
    if (!this.apiKey) {
      throw new TranscriptionError("OpenAI API key not configured", TranscriptionErrorCode.API_ERROR);
    }
    try {
      const formData = new FormData();
      formData.append("file", audioBlob, "audio.wav");
      formData.append("model", "whisper-1");
      formData.append("response_format", "json");
      const response = await fetch(this.baseUrl, {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`
        },
        body: formData
      });
      if (!response.ok) {
        const error = await response.json().catch(() => ({ error: { message: "Unknown API error" } }));
        throw new TranscriptionError(`API Error: ${((_a = error.error) == null ? void 0 : _a.message) || response.statusText}`, TranscriptionErrorCode.API_ERROR);
      }
      const result = await response.json();
      if (!result.text) {
        throw new TranscriptionError("API returned empty transcription", TranscriptionErrorCode.API_ERROR);
      }
      return result.text;
    } catch (error) {
      if (error instanceof TranscriptionError) {
        throw error;
      }
      console.error("Whisper API error:", error);
      throw new TranscriptionError("Failed to transcribe audio", TranscriptionErrorCode.API_ERROR, error);
    }
  }
  updateApiKey(newKey) {
    this.apiKey = newKey;
  }
  async validateApiKey() {
    if (!this.apiKey) {
      throw new TranscriptionError("OpenAI API key not configured", TranscriptionErrorCode.API_ERROR);
    }
    try {
      const response = await fetch("https://api.openai.com/v1/models", {
        headers: {
          "Authorization": `Bearer ${this.apiKey}`
        }
      });
      if (!response.ok) {
        throw new TranscriptionError("Invalid API key", TranscriptionErrorCode.API_ERROR);
      }
    } catch (error) {
      throw new TranscriptionError("Failed to validate API key", TranscriptionErrorCode.API_ERROR, error);
    }
  }
};

// src/services/whisper-local-service.ts
var import_meta = {};
var WhisperLocalService = class {
  constructor(plugin) {
    this.isInitialized = false;
    this.modelConfig = null;
    this.worker = null;
    this.workerReady = null;
    this.plugin = plugin;
  }
  async initialize() {
    if (this.isInitialized) {
      return;
    }
    try {
      this.worker = new Worker(new URL("../workers/whisper-worker.ts", import_meta.url), { type: "module" });
      this.worker.onmessage = this.handleWorkerMessage.bind(this);
      this.worker.onerror = this.handleWorkerError.bind(this);
      this.workerReady = new Promise((resolve, reject) => {
        var _a, _b, _c, _d;
        if (!this.worker) {
          reject(new Error("Worker not created"));
          return;
        }
        const config = {
          modelPath: ((_a = this.modelConfig) == null ? void 0 : _a.modelPath) || "models/whisper-base.bin",
          device: ((_b = this.modelConfig) == null ? void 0 : _b.device) || "cpu",
          threads: ((_c = this.modelConfig) == null ? void 0 : _c.threads) || navigator.hardwareConcurrency || 4,
          language: (_d = this.modelConfig) == null ? void 0 : _d.language
        };
        const handleInitMessage = (event) => {
          var _a2, _b2, _c2;
          if (event.data.type === "initialized") {
            (_a2 = this.worker) == null ? void 0 : _a2.removeEventListener("message", handleInitMessage);
            this.isInitialized = true;
            resolve();
          } else if (event.data.type === "error") {
            (_b2 = this.worker) == null ? void 0 : _b2.removeEventListener("message", handleInitMessage);
            reject(new Error((_c2 = event.data.error) == null ? void 0 : _c2.message));
          }
        };
        this.worker.addEventListener("message", handleInitMessage);
        this.worker.postMessage({ type: "initialize", config });
      });
      await this.workerReady;
    } catch (error) {
      if (error instanceof TranscriptionError) {
        throw error;
      }
      throw new TranscriptionError("Failed to initialize local Whisper model", TranscriptionErrorCode.LOCAL_MODEL_ERROR, error);
    }
  }
  async transcribe(audioBlob, onProgress) {
    if (!this.isInitialized || !this.worker) {
      await this.initialize();
    }
    if (!this.worker) {
      throw new TranscriptionError("Worker not initialized", TranscriptionErrorCode.LOCAL_MODEL_ERROR);
    }
    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const transcription = await new Promise((resolve, reject) => {
        if (!this.worker) {
          reject(new Error("Worker not initialized"));
          return;
        }
        const handleTranscribeMessage = (event) => {
          var _a, _b, _c, _d, _e;
          switch (event.data.type) {
            case "transcribed":
              (_a = this.worker) == null ? void 0 : _a.removeEventListener("message", handleTranscribeMessage);
              resolve(event.data.text || "");
              break;
            case "error":
              (_b = this.worker) == null ? void 0 : _b.removeEventListener("message", handleTranscribeMessage);
              const error = new TranscriptionError(((_c = event.data.error) == null ? void 0 : _c.message) || "Unknown error", ((_d = event.data.error) == null ? void 0 : _d.code) || TranscriptionErrorCode.UNKNOWN, (_e = event.data.error) == null ? void 0 : _e.details);
              reject(error);
              break;
            case "progress":
              if (onProgress && event.data.progress) {
                onProgress(event.data.progress);
              }
              break;
          }
        };
        this.worker.addEventListener("message", handleTranscribeMessage);
        this.worker.postMessage({ type: "transcribe", audioData: arrayBuffer });
      });
      return transcription;
    } catch (error) {
      if (error instanceof TranscriptionError) {
        throw error;
      }
      throw new TranscriptionError("Failed to transcribe audio using local model", TranscriptionErrorCode.LOCAL_MODEL_ERROR, error);
    }
  }
  async updateConfig(config) {
    if (!this.modelConfig) {
      this.modelConfig = {
        modelPath: "models/whisper-base.bin",
        device: "cpu",
        threads: navigator.hardwareConcurrency || 4,
        maxMemoryMB: 1024
      };
    }
    if (config.threads && config.threads < 1) {
      throw new TranscriptionError("Thread count must be at least 1", TranscriptionErrorCode.INVALID_OPTIONS);
    }
    if (config.maxMemoryMB && config.maxMemoryMB < 256) {
      throw new TranscriptionError("Memory limit must be at least 256MB", TranscriptionErrorCode.INVALID_OPTIONS);
    }
    Object.assign(this.modelConfig, config);
    if (this.isInitialized) {
      await this.cleanup();
      await this.initialize();
    }
  }
  async getStatus() {
    if (!this.worker) {
      return {
        initialized: false,
        memoryUsage: { heapUsed: 0, heapTotal: 0 }
      };
    }
    return new Promise((resolve) => {
      const worker = this.worker;
      if (!worker) {
        resolve({
          initialized: false,
          memoryUsage: { heapUsed: 0, heapTotal: 0 }
        });
        return;
      }
      const handleStatusMessage = (event) => {
        if (event.data.type === "status") {
          worker.removeEventListener("message", handleStatusMessage);
          resolve(event.data.status || {
            initialized: false,
            memoryUsage: { heapUsed: 0, heapTotal: 0 }
          });
        }
      };
      worker.addEventListener("message", handleStatusMessage);
      worker.postMessage({ type: "status" });
    });
  }
  handleWorkerMessage(event) {
    const { type, error, progress } = event.data;
    switch (type) {
      case "error":
        if (error == null ? void 0 : error.message) {
          console.error("Worker error:", error.message);
          this.plugin.setStatusBarText(`Error: ${error.message}`, 5e3);
        }
        break;
      case "progress":
        if ((progress == null ? void 0 : progress.stage) === "processing" && typeof progress.percent === "number") {
          this.plugin.setStatusBarText(`Transcribing: ${progress.percent}%`, 0);
        }
        break;
      case "status":
        break;
    }
  }
  handleWorkerError(error) {
    console.error("Worker error:", error);
    this.cleanup();
  }
  async cleanup() {
    if (!this.worker) {
      return;
    }
    try {
      await new Promise((resolve, reject) => {
        const cleanup = () => {
          if (this.worker) {
            this.worker.terminate();
            this.worker = null;
          }
          this.isInitialized = false;
          this.workerReady = null;
          resolve();
        };
        const currentWorker = this.worker;
        if (!currentWorker) {
          cleanup();
          return;
        }
        const handleCleanupMessage = (event) => {
          if (event.data.type === "status" && event.data.status && !event.data.status.initialized) {
            currentWorker.removeEventListener("message", handleCleanupMessage);
            currentWorker.removeEventListener("error", handleCleanupError);
            cleanup();
          }
        };
        const handleCleanupError = (error) => {
          currentWorker.removeEventListener("message", handleCleanupMessage);
          currentWorker.removeEventListener("error", handleCleanupError);
          console.error("Cleanup error:", error);
          cleanup();
          reject(error);
        };
        currentWorker.addEventListener("message", handleCleanupMessage);
        currentWorker.addEventListener("error", handleCleanupError);
        currentWorker.postMessage({ type: "cleanup" });
        setTimeout(() => {
          cleanup();
          reject(new Error("Cleanup timeout"));
        }, 5e3);
      });
    } catch (error) {
      console.error("Failed to cleanup worker:", error);
      if (this.worker) {
        this.worker.terminate();
        this.worker = null;
      }
      this.isInitialized = false;
      this.workerReady = null;
    }
  }
  isReady() {
    return this.isInitialized && !!this.worker;
  }
};

// src/services/text-analysis-service.ts
var TextAnalysisService = class {
  constructor(plugin) {
    this.taskPatterns = [];
    this.plugin = plugin;
    this.initializePatterns();
  }
  initializePatterns() {
    this.taskPatterns = [
      /(?:high priority|urgent|asap):?\s*([^.!?\n]+[.!?\n])/gi,
      /(?:todo|task|action item):?\s*([^.!?\n]+[.!?\n])/gi,
      /(?:need to|have to|must)\s+([^.!?\n]+[.!?\n])/gi,
      /(?:remember to|don't forget to)\s+([^.!?\n]+[.!?\n])/gi,
      /(?:by|before|due)\s+(?:monday|tuesday|wednesday|thursday|friday|saturday|sunday|tomorrow|next week)\s*[,:]\s*([^.!?\n]+[.!?\n])/gi,
      /(?:assigned to|delegate to)\s+\w+\s*[,:]\s*([^.!?\n]+[.!?\n])/gi
    ];
  }
  analyze(text) {
    const tasks = this.extractTasks(text);
    return {
      tasks,
      keyPoints: this.extractKeyPoints(text)
    };
  }
  extractTasks(text) {
    var _a;
    const tasks = [];
    const processedLines = new Set();
    for (const pattern of this.taskPatterns) {
      const matches = text.matchAll(pattern);
      for (const match of matches) {
        const taskText = (_a = match[1]) == null ? void 0 : _a.trim();
        if (taskText && !processedLines.has(taskText.toLowerCase())) {
          processedLines.add(taskText.toLowerCase());
          const task = {
            text: taskText,
            priority: this.detectPriority(match[0]),
            dueDate: this.extractDate(match[0]),
            tags: this.extractTags(taskText),
            context: this.extractContext(text, match.index || 0)
          };
          tasks.push(task);
        }
      }
    }
    return tasks;
  }
  detectPriority(text) {
    const lowercaseText = text.toLowerCase();
    if (/urgent|asap|high priority|critical|immediately/i.test(lowercaseText)) {
      return "high";
    }
    if (/soon|important|medium priority/i.test(lowercaseText)) {
      return "medium";
    }
    if (/eventually|when possible|low priority/i.test(lowercaseText)) {
      return "low";
    }
    return void 0;
  }
  extractDate(text) {
    const dateMatch = text.match(/(?:by|before|due|on)\s+((?:monday|tuesday|wednesday|thursday|friday|saturday|sunday|tomorrow|next week)|(?:\d{1,2}\/\d{1,2}(?:\/\d{2,4})?)|(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\s+\d{1,2}(?:st|nd|rd|th)?(?:,?\s+\d{4})?)/i);
    if (dateMatch) {
      return dateMatch[1];
    }
    return void 0;
  }
  extractTags(text) {
    const tags = [];
    const hashTags = text.match(/#[\w-]+/g);
    if (hashTags) {
      tags.push(...hashTags);
    }
    const mentions = text.match(/@[\w-]+/g);
    if (mentions) {
      tags.push(...mentions);
    }
    return tags;
  }
  extractContext(fullText, matchIndex) {
    const contextWindow = 100;
    const start = Math.max(0, matchIndex - contextWindow);
    const end = Math.min(fullText.length, matchIndex + contextWindow);
    return fullText.slice(start, end).trim();
  }
  extractKeyPoints(text) {
    const keyPoints = [];
    const sentences = text.match(/[^.!?\n]+[.!?\n]+/g) || [];
    for (const sentence of sentences) {
      if (this.isKeyPoint(sentence)) {
        keyPoints.push(sentence.trim());
      }
    }
    return keyPoints;
  }
  isKeyPoint(sentence) {
    const keyPhrases = [
      /\b(?:key|main|important|significant|notable)\s+point\b/i,
      /\b(?:primarily|mainly|essentially|fundamentally)\b/i,
      /\b(?:in\s+summary|to\s+summarize|in\s+conclusion)\b/i,
      /\b(?:highlight|emphasize|stress|note)\b/i
    ];
    return keyPhrases.some((pattern) => pattern.test(sentence));
  }
  formatTasks(tasks) {
    return tasks.map((task) => {
      const parts = [`- [ ] ${task.text}`];
      if (task.priority) {
        parts.push(`[Priority: ${task.priority}]`);
      }
      if (task.dueDate) {
        parts.push(`[Due: ${task.dueDate}]`);
      }
      if (task.tags && task.tags.length > 0) {
        parts.push(task.tags.join(" "));
      }
      return parts.join(" ");
    });
  }
};

// src/services/summarization-service.ts
var SummarizationService = class {
  constructor(plugin) {
    this.CHUNK_SIZE = 1e3;
    this.CHUNK_OVERLAP = 200;
    this.plugin = plugin;
  }
  async summarize(text, options = {}) {
    try {
      const chunks = this.chunkText(text);
      const chunkSummaries = await Promise.all(chunks.map((chunk) => this.summarizeChunk(chunk, options)));
      const combinedSummary = this.combineChunkSummaries(chunkSummaries);
      return {
        summary: combinedSummary,
        topics: this.extractTopics(text),
        decisions: this.extractDecisions(text),
        questions: this.extractQuestions(text)
      };
    } catch (error) {
      console.error("Summarization failed:", error);
      throw new TranscriptionError("Failed to generate summary", TranscriptionErrorCode.UNKNOWN, error);
    }
  }
  chunkText(text) {
    if (!text || text.length === 0) {
      return [];
    }
    const chunks = [];
    let startIndex = 0;
    while (startIndex < text.length) {
      const endIndex = Math.min(startIndex + this.CHUNK_SIZE, text.length);
      let breakPoint = endIndex;
      if (endIndex < text.length) {
        const searchStart = Math.max(startIndex, endIndex - 50);
        const nextPeriod = text.indexOf(".", searchStart);
        if (nextPeriod !== -1 && nextPeriod < endIndex + 50) {
          breakPoint = nextPeriod + 1;
        }
      }
      const chunk = text.slice(startIndex, breakPoint).trim();
      if (chunk) {
        chunks.push(chunk);
      }
      startIndex = Math.min(breakPoint, text.length);
      if (startIndex < text.length) {
        startIndex = Math.max(startIndex - this.CHUNK_OVERLAP, 0);
      }
    }
    return chunks;
  }
  async summarizeChunk(chunk, options) {
    return chunk.split(".")[0] + ".";
  }
  combineChunkSummaries(summaries) {
    return summaries.join("\n\n");
  }
  extractTopics(text) {
    const topics = new Set();
    const topicPatterns = [
      /discussed?\s+(\w+(?:\s+\w+){0,3})/gi,
      /regarding\s+(\w+(?:\s+\w+){0,3})/gi,
      /about\s+(\w+(?:\s+\w+){0,3})/gi,
      /topic:\s*(\w+(?:\s+\w+){0,3})/gi
    ];
    for (const pattern of topicPatterns) {
      const matches = text.matchAll(pattern);
      for (const match of matches) {
        if (match[1]) {
          topics.add(match[1].trim());
        }
      }
    }
    return Array.from(topics);
  }
  extractDecisions(text) {
    const decisions = new Set();
    const decisionPatterns = [
      /decided\s+to\s+([^.!?\n]+[.!?\n])/gi,
      /decision:\s*([^.!?\n]+[.!?\n])/gi,
      /agreed\s+to\s+([^.!?\n]+[.!?\n])/gi,
      /conclusion:\s*([^.!?\n]+[.!?\n])/gi
    ];
    for (const pattern of decisionPatterns) {
      const matches = text.matchAll(pattern);
      for (const match of matches) {
        if (match[1]) {
          decisions.add(match[1].trim());
        }
      }
    }
    return Array.from(decisions);
  }
  extractQuestions(text) {
    const questions = new Set();
    const sentences = text.match(/[^.!?\n]+[.!?\n]+/g) || [];
    for (const sentence of sentences) {
      if (sentence.trim().endsWith("?") || /^(?:what|who|where|when|why|how)\b/i.test(sentence.trim())) {
        questions.add(sentence.trim());
      }
    }
    return Array.from(questions);
  }
  formatSummary(result) {
    const parts = [
      "## Summary",
      "",
      result.summary,
      ""
    ];
    if (result.topics && result.topics.length > 0) {
      parts.push("", "### Topics Discussed", ...result.topics.map((topic) => `- ${topic}`), "");
    }
    if (result.decisions && result.decisions.length > 0) {
      parts.push("", "### Decisions Made", ...result.decisions.map((decision) => `- ${decision}`), "");
    }
    if (result.questions && result.questions.length > 0) {
      parts.push("", "### Questions Raised", ...result.questions.map((question) => `- ${question}`), "");
    }
    return parts.join("\n");
  }
};

// src/services/transcription-service.ts
var TranscriptionService = class {
  constructor(plugin) {
    this.queue = [];
    this.isProcessing = false;
    this.plugin = plugin;
    this.apiClient = new WhisperAPIClient(this.plugin.settings.openaiApiKey);
    this.localService = new WhisperLocalService(this.plugin);
    this.analysisService = new TextAnalysisService(this.plugin);
    this.summarizationService = new SummarizationService(this.plugin);
  }
  async transcribe(audioBlob) {
    const jobId = `job-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    const job = {
      id: jobId,
      audioBlob,
      timestamp: Date.now(),
      status: "pending"
    };
    this.queue.push(job);
    this.processQueue();
    return jobId;
  }
  async processQueue() {
    if (this.isProcessing || this.queue.length === 0) {
      return;
    }
    if (this.queue.length >= 10) {
      throw new TranscriptionError("Transcription queue is full. Please wait for current jobs to complete.", TranscriptionErrorCode.QUEUE_FULL);
    }
    this.isProcessing = true;
    const job = this.queue[0];
    job.status = "processing";
    try {
      const transcription = await this.processTranscription(job.audioBlob);
      job.status = "completed";
      job.result = transcription;
      await this.createTranscriptionNote(job);
      new import_obsidian2.Notice("Transcription completed");
    } catch (error) {
      console.error("Transcription failed:", error);
      job.status = "failed";
      if (error instanceof TranscriptionError) {
        job.error = `${error.code}: ${error.message}`;
      } else {
        job.error = error instanceof Error ? error.message : "Unknown error";
        error = new TranscriptionError(job.error, TranscriptionErrorCode.UNKNOWN, error);
      }
      new import_obsidian2.Notice(`Transcription failed: ${job.error}`);
      throw error;
    }
    this.queue.shift();
    this.isProcessing = false;
    this.processQueue();
  }
  async processTranscription(audioBlob) {
    const settings = this.plugin.settings;
    if (settings.transcriptionModel === "local") {
      return this.processLocalTranscription(audioBlob);
    } else {
      return this.processApiTranscription(audioBlob);
    }
  }
  async processLocalTranscription(audioBlob) {
    try {
      return await this.localService.transcribe(audioBlob);
    } catch (error) {
      if (error instanceof TranscriptionError) {
        throw error;
      }
      throw new TranscriptionError("Local transcription failed", TranscriptionErrorCode.LOCAL_MODEL_ERROR, error);
    }
  }
  async processApiTranscription(audioBlob) {
    if (!this.plugin.settings.openaiApiKey) {
      throw new TranscriptionError("OpenAI API key not configured. Please add your API key in settings.", TranscriptionErrorCode.API_ERROR);
    }
    return this.apiClient.transcribe(audioBlob);
  }
  updateSettings(settings) {
    this.apiClient.updateApiKey(settings.openaiApiKey);
    if (settings.transcriptionModel === "local") {
      this.localService.updateConfig({
        threads: navigator.hardwareConcurrency || 4,
        device: "cpu"
      });
    }
  }
  async validateApiKey() {
    await this.apiClient.validateApiKey();
  }
  async createTranscriptionNote(job) {
    if (!job.result) {
      throw new TranscriptionError("No transcription result available", TranscriptionErrorCode.NOTE_CREATION_FAILED);
    }
    const analysis = this.analysisService.analyze(job.result);
    let summary;
    if (this.plugin.settings.summarization.enabled) {
      try {
        summary = await this.summarizationService.summarize(job.result, {
          maxLength: this.plugin.settings.summarization.maxLength,
          style: this.plugin.settings.summarization.style,
          focusAreas: Object.entries(this.plugin.settings.summarization.includeSections).filter(([_, enabled]) => enabled).map(([area]) => area)
        });
      } catch (error) {
        console.error("Summary generation failed:", error);
        const summaryError = error instanceof SummarizationError ? error : new SummarizationError("Failed to generate summary", SummarizationErrorCode.UNKNOWN, error);
        new import_obsidian2.Notice(`Summary generation failed: ${summaryError.message}`);
      }
    }
    const timestamp = new Date(job.timestamp);
    const safeTimestamp = timestamp.toISOString().replace(/:/g, "-").replace(/\./g, "-").slice(0, 19);
    const fileName = `voice-memo-${safeTimestamp}.md`;
    const contentParts = [
      "---",
      "type: voice-memo",
      `created: ${timestamp.toISOString()}`,
      "---",
      "",
      "# Voice Memo Transcription",
      ""
    ];
    if (summary) {
      contentParts.push(this.summarizationService.formatSummary(summary), "", "## Full Transcription", "", job.result, "");
    } else {
      contentParts.push(job.result, "");
    }
    if (this.plugin.settings.analysis.extractTasks && analysis.tasks.length > 0) {
      contentParts.push("", "## Tasks", ...this.analysisService.formatTasks(analysis.tasks), "");
    }
    if (this.plugin.settings.analysis.extractKeyPoints && analysis.keyPoints && analysis.keyPoints.length > 0) {
      contentParts.push("", "## Key Points", ...analysis.keyPoints.map((point) => `- ${point}`), "");
    }
    await this.plugin.app.vault.create(`${this.plugin.settings.memoStoragePath}/${fileName}`, contentParts.join("\n"));
  }
  getJobStatus(jobId) {
    const job = this.queue.find((j) => j.id === jobId);
    if (!job) {
      return null;
    }
    return {
      status: job.status,
      result: job.result,
      error: job.error
    };
  }
  cleanup() {
    this.queue = [];
    this.isProcessing = false;
    this.localService.cleanup();
  }
};

// src/managers/ui-manager.ts
var UiManager = class {
  constructor(plugin) {
    this.ribbonIcon = null;
    this.statusBarItem = null;
    this.statusBarTimeout = null;
    this.plugin = plugin;
  }
  addRibbonIcon(onClick) {
    this.ribbonIcon = this.plugin.addRibbonIcon("mic", "Start Voice Recording", onClick);
  }
  setRibbonIcon(isRecording) {
    if (!this.ribbonIcon) {
      console.warn("Ribbon icon not initialized");
      return;
    }
    try {
      if (isRecording) {
        this.ribbonIcon.innerHTML = '<svg viewBox="0 0 100 100" class="svg-icon"><rect width="100" height="100"/></svg>';
        this.ribbonIcon.setAttribute("aria-label", "Stop Recording");
        this.ribbonIcon.addClass("recording-active");
      } else {
        this.ribbonIcon.innerHTML = '<svg viewBox="0 0 100 100" class="svg-icon"><circle cx="50" cy="50" r="40"/></svg>';
        this.ribbonIcon.setAttribute("aria-label", "Start Voice Recording");
        this.ribbonIcon.removeClass("recording-active");
      }
    } catch (error) {
      console.error("Error updating ribbon icon:", error);
    }
  }
  setStatusBarText(text, timeout = 0) {
    if (!this.statusBarItem) {
      this.statusBarItem = this.plugin.addStatusBarItem();
    }
    this.statusBarItem.setText(text);
    if (this.statusBarTimeout) {
      clearTimeout(this.statusBarTimeout);
      this.statusBarTimeout = null;
    }
    if (timeout > 0) {
      this.statusBarTimeout = setTimeout(() => {
        if (this.statusBarItem) {
          this.statusBarItem.setText("");
        }
      }, timeout);
    }
  }
  cleanup() {
    if (this.statusBarTimeout) {
      clearTimeout(this.statusBarTimeout);
      this.statusBarTimeout = null;
    }
    if (this.statusBarItem) {
      this.statusBarItem.remove();
      this.statusBarItem = null;
    }
    if (this.ribbonIcon) {
      this.ribbonIcon.remove();
      this.ribbonIcon = null;
    }
  }
};

// src/managers/command-manager.ts
var import_obsidian3 = __toModule(require("obsidian"));
var CommandManager = class {
  constructor(plugin, voiceRecorderManager, uiManager) {
    this.plugin = plugin;
    this.voiceRecorderManager = voiceRecorderManager;
    this.uiManager = uiManager;
  }
  registerCommands() {
    this.registerStartRecordingCommand();
    this.registerStopRecordingCommand();
  }
  handleError(error) {
    if (error instanceof VoiceRecorderError) {
      new import_obsidian3.Notice(error.message);
    } else {
      console.error("Unexpected error:", error);
      new import_obsidian3.Notice("An unexpected error occurred");
    }
    this.uiManager.setRibbonIcon(false);
  }
  registerStartRecordingCommand() {
    this.plugin.addCommand({
      id: "start-voice-memo",
      name: "Start Voice Memo",
      hotkeys: [{ modifiers: ["Mod", "Shift"], key: "r" }],
      callback: async () => {
        try {
          await this.voiceRecorderManager.startRecording();
          this.uiManager.setRibbonIcon(true);
        } catch (error) {
          this.handleError(error);
        }
      }
    });
  }
  registerStopRecordingCommand() {
    this.plugin.addCommand({
      id: "stop-voice-memo",
      name: "Stop Voice Memo",
      hotkeys: [{ modifiers: ["Mod", "Shift"], key: "s" }],
      callback: async () => {
        try {
          await this.voiceRecorderManager.stopRecording();
          this.uiManager.setRibbonIcon(false);
        } catch (error) {
          this.handleError(error);
        }
      }
    });
  }
  setupRibbonIcon() {
    this.uiManager.addRibbonIcon(async () => {
      try {
        if (this.voiceRecorderManager.isRecording()) {
          await this.voiceRecorderManager.stopRecording();
          this.uiManager.setRibbonIcon(false);
        } else {
          await this.voiceRecorderManager.startRecording();
          this.uiManager.setRibbonIcon(true);
        }
      } catch (error) {
        this.handleError(error);
      }
    });
  }
};

// src/managers/settings-manager.ts
var import_obsidian5 = __toModule(require("obsidian"));

// src/types/settings.ts
var DEFAULT_SETTINGS = {
  transcriptionModel: "whisper-1",
  autoTranscribe: true,
  saveAudioFiles: true,
  memoStoragePath: "voice-memos",
  audioQuality: "medium",
  openaiApiKey: "",
  validateApiKey: true,
  localModel: {
    modelPath: "models/whisper-base.bin",
    device: "cpu",
    threads: navigator.hardwareConcurrency || 4
  },
  analysis: {
    extractTasks: true,
    extractKeyPoints: true,
    includeTaskContext: true,
    includeTaskPriority: true,
    includeTaskDates: true,
    taskKeywords: [
      "todo",
      "task",
      "action item",
      "need to",
      "have to",
      "must",
      "remember to",
      "don't forget"
    ]
  },
  summarization: {
    enabled: true,
    style: "detailed",
    maxLength: 500,
    includeSections: {
      topics: true,
      decisions: true,
      questions: true
    },
    chunkSize: 1e3,
    chunkOverlap: 200
  }
};

// src/utils/error-handler.ts
var import_obsidian4 = __toModule(require("obsidian"));
var ErrorHandler = class {
  static handleError(error) {
    if (error instanceof VoiceRecorderError || error instanceof TranscriptionError) {
      new import_obsidian4.Notice(error.message);
    } else {
      console.error("Unexpected error:", error);
      new import_obsidian4.Notice("An unexpected error occurred");
    }
  }
  static logError(error, context) {
    const message = error instanceof Error ? error.message : "Unknown error";
    if (context) {
      console.error(`[${context}] ${message}`, error);
    } else {
      console.error(message, error);
    }
  }
  static showNotice(message) {
    new import_obsidian4.Notice(message);
  }
};

// src/managers/settings-manager.ts
var AiVoiceMemoSettingTab = class extends import_obsidian5.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  async display() {
    const { containerEl } = this;
    containerEl.empty();
    this.renderApiConfiguration(containerEl);
    this.renderRecordingConfiguration(containerEl);
    this.renderTranscriptionConfiguration(containerEl);
    this.renderAnalysisConfiguration(containerEl);
    this.renderSummarizationConfiguration(containerEl);
  }
  renderApiConfiguration(containerEl) {
    containerEl.createEl("h3", { text: "API Configuration" });
    const apiKeySetting = new import_obsidian5.Setting(containerEl).setName("OpenAI API Key").setDesc("Your OpenAI API key for Whisper transcription").addText((text) => text.setPlaceholder("sk-...").setValue(this.plugin.settings.openaiApiKey).onChange(async (value) => {
      this.plugin.settings.openaiApiKey = value;
      await this.plugin.saveSettings();
      this.plugin.transcriptionService.updateSettings(this.plugin.settings);
    }).inputEl.setAttribute("type", "password"));
    apiKeySetting.addButton((button) => button.setButtonText("Validate").onClick(async () => {
      button.setDisabled(true);
      try {
        await this.plugin.transcriptionService.validateApiKey();
        new import_obsidian5.Notice("API key is valid");
      } catch (error) {
        if (error instanceof TranscriptionError) {
          new import_obsidian5.Notice(error.message);
        } else {
          new import_obsidian5.Notice("Failed to validate API key");
        }
      } finally {
        button.setDisabled(false);
      }
    }));
    new import_obsidian5.Setting(containerEl).setName("Validate API Key on Change").setDesc("Automatically validate API key when it is changed").addToggle((toggle) => toggle.setValue(this.plugin.settings.validateApiKey).onChange(async (value) => {
      this.plugin.settings.validateApiKey = value;
      await this.plugin.saveSettings();
    }));
  }
  renderRecordingConfiguration(containerEl) {
    containerEl.createEl("h3", { text: "Recording Configuration" });
    new import_obsidian5.Setting(containerEl).setName("Audio Quality").setDesc("Set the quality level for audio recording").addDropdown((dropdown) => dropdown.addOptions({
      "low": "Low (Smaller Files)",
      "medium": "Medium (Balanced)",
      "high": "High (Better Quality)"
    }).setValue(this.plugin.settings.audioQuality).onChange(async (value) => {
      this.plugin.settings.audioQuality = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Storage Path").setDesc("Path where voice memos will be stored").addText((text) => text.setPlaceholder("voice-memos").setValue(this.plugin.settings.memoStoragePath).onChange(async (value) => {
      this.plugin.settings.memoStoragePath = value;
      await this.plugin.saveSettings();
    }));
  }
  renderTranscriptionConfiguration(containerEl) {
    containerEl.createEl("h3", { text: "Transcription Configuration" });
    const modelSetting = new import_obsidian5.Setting(containerEl).setName("Transcription Model").setDesc("Select the Whisper model to use for transcription").addDropdown((dropdown) => dropdown.addOptions({
      "whisper-1": "OpenAI Whisper (Cloud)",
      "local": "Local Whisper Model"
    }).setValue(this.plugin.settings.transcriptionModel).onChange(async (value) => {
      this.plugin.settings.transcriptionModel = value;
      await this.plugin.saveSettings();
      this.display();
    }));
    if (this.plugin.settings.transcriptionModel === "local") {
      this.renderLocalModelConfiguration(containerEl);
    }
    new import_obsidian5.Setting(containerEl).setName("Auto-Transcribe").setDesc("Automatically transcribe recordings when they finish").addToggle((toggle) => toggle.setValue(this.plugin.settings.autoTranscribe).onChange(async (value) => {
      this.plugin.settings.autoTranscribe = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Save Audio Files").setDesc("Keep the audio files after transcription").addToggle((toggle) => toggle.setValue(this.plugin.settings.saveAudioFiles).onChange(async (value) => {
      this.plugin.settings.saveAudioFiles = value;
      await this.plugin.saveSettings();
    }));
  }
  renderLocalModelConfiguration(containerEl) {
    containerEl.createEl("h3", { text: "Local Model Configuration" });
    new import_obsidian5.Setting(containerEl).setName("Model Path").setDesc("Path to the Whisper model file").addText((text) => text.setPlaceholder("models/whisper-base.bin").setValue(this.plugin.settings.localModel.modelPath).onChange(async (value) => {
      this.plugin.settings.localModel.modelPath = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Processing Device").setDesc("Select the device to use for transcription").addDropdown((dropdown) => dropdown.addOptions({
      "cpu": "CPU",
      "gpu": "GPU (if available)"
    }).setValue(this.plugin.settings.localModel.device).onChange(async (value) => {
      this.plugin.settings.localModel.device = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Thread Count").setDesc("Number of threads to use for processing (default: auto)").addSlider((slider) => slider.setLimits(1, Math.max(navigator.hardwareConcurrency || 4, 1), 1).setValue(this.plugin.settings.localModel.threads).setDynamicTooltip().onChange(async (value) => {
      this.plugin.settings.localModel.threads = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Language").setDesc("Optional: Specify language for better accuracy (leave empty for auto-detect)").addText((text) => text.setPlaceholder("en, fr, de, etc.").setValue(this.plugin.settings.localModel.language || "").onChange(async (value) => {
      this.plugin.settings.localModel.language = value || void 0;
      await this.plugin.saveSettings();
    }));
  }
  renderAnalysisConfiguration(containerEl) {
    containerEl.createEl("h3", { text: "Analysis Configuration" });
    new import_obsidian5.Setting(containerEl).setName("Extract Tasks").setDesc("Automatically identify and extract tasks from transcriptions").addToggle((toggle) => toggle.setValue(this.plugin.settings.analysis.extractTasks).onChange(async (value) => {
      this.plugin.settings.analysis.extractTasks = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Extract Key Points").setDesc("Identify and extract key points from transcriptions").addToggle((toggle) => toggle.setValue(this.plugin.settings.analysis.extractKeyPoints).onChange(async (value) => {
      this.plugin.settings.analysis.extractKeyPoints = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Include Task Context").setDesc("Include surrounding context with extracted tasks").addToggle((toggle) => toggle.setValue(this.plugin.settings.analysis.includeTaskContext).onChange(async (value) => {
      this.plugin.settings.analysis.includeTaskContext = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Include Task Priority").setDesc("Detect and include task priority levels").addToggle((toggle) => toggle.setValue(this.plugin.settings.analysis.includeTaskPriority).onChange(async (value) => {
      this.plugin.settings.analysis.includeTaskPriority = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Include Task Dates").setDesc("Extract and include due dates from tasks").addToggle((toggle) => toggle.setValue(this.plugin.settings.analysis.includeTaskDates).onChange(async (value) => {
      this.plugin.settings.analysis.includeTaskDates = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Task Keywords").setDesc("Keywords used to identify tasks (comma-separated)").addTextArea((text) => text.setPlaceholder("todo, task, need to, have to, must").setValue(this.plugin.settings.analysis.taskKeywords.join(", ")).onChange(async (value) => {
      this.plugin.settings.analysis.taskKeywords = value.split(",").map((keyword) => keyword.trim()).filter((keyword) => keyword.length > 0);
      await this.plugin.saveSettings();
    }));
  }
  renderSummarizationConfiguration(containerEl) {
    containerEl.createEl("h3", { text: "Summarization Configuration" });
    new import_obsidian5.Setting(containerEl).setName("Enable Summaries").setDesc("Automatically generate summaries for voice memos").addToggle((toggle) => toggle.setValue(this.plugin.settings.summarization.enabled).onChange(async (value) => {
      this.plugin.settings.summarization.enabled = value;
      await this.plugin.saveSettings();
      this.display();
    }));
    if (this.plugin.settings.summarization.enabled) {
      this.renderSummarizationDetails(containerEl);
    }
  }
  renderSummarizationDetails(containerEl) {
    new import_obsidian5.Setting(containerEl).setName("Summary Style").setDesc("Choose how summaries should be formatted").addDropdown((dropdown) => dropdown.addOptions({
      "concise": "Concise (Brief overview)",
      "detailed": "Detailed (Comprehensive)",
      "bullet-points": "Bullet Points (Key items)"
    }).setValue(this.plugin.settings.summarization.style).onChange(async (value) => {
      this.plugin.settings.summarization.style = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Maximum Length").setDesc("Maximum length of generated summaries (in characters)").addSlider((slider) => slider.setLimits(100, 1e3, 100).setValue(this.plugin.settings.summarization.maxLength).setDynamicTooltip().onChange(async (value) => {
      this.plugin.settings.summarization.maxLength = value;
      await this.plugin.saveSettings();
    }));
    this.renderSummarizationSections(containerEl);
    this.renderAdvancedSummarizationSettings(containerEl);
  }
  renderSummarizationSections(containerEl) {
    new import_obsidian5.Setting(containerEl).setName("Include Topics").setDesc("Extract and list main topics discussed").addToggle((toggle) => toggle.setValue(this.plugin.settings.summarization.includeSections.topics).onChange(async (value) => {
      this.plugin.settings.summarization.includeSections.topics = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Include Decisions").setDesc("Extract and list decisions made").addToggle((toggle) => toggle.setValue(this.plugin.settings.summarization.includeSections.decisions).onChange(async (value) => {
      this.plugin.settings.summarization.includeSections.decisions = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Include Questions").setDesc("Extract and list questions raised").addToggle((toggle) => toggle.setValue(this.plugin.settings.summarization.includeSections.questions).onChange(async (value) => {
      this.plugin.settings.summarization.includeSections.questions = value;
      await this.plugin.saveSettings();
    }));
  }
  renderAdvancedSummarizationSettings(containerEl) {
    new import_obsidian5.Setting(containerEl).setName("Advanced: Chunk Size").setDesc("Size of text chunks for processing (in characters)").addSlider((slider) => slider.setLimits(500, 2e3, 100).setValue(this.plugin.settings.summarization.chunkSize).setDynamicTooltip().onChange(async (value) => {
      this.plugin.settings.summarization.chunkSize = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian5.Setting(containerEl).setName("Advanced: Chunk Overlap").setDesc("Overlap between text chunks for better context").addSlider((slider) => slider.setLimits(50, 500, 50).setValue(this.plugin.settings.summarization.chunkOverlap).setDynamicTooltip().onChange(async (value) => {
      this.plugin.settings.summarization.chunkOverlap = value;
      await this.plugin.saveSettings();
    }));
  }
};
var SettingsManager = class {
  constructor(plugin) {
    this.settingsTab = null;
    this.plugin = plugin;
  }
  async loadSettings() {
    try {
      this.plugin.settings = Object.assign({}, DEFAULT_SETTINGS, await this.plugin.loadData());
    } catch (error) {
      ErrorHandler.logError(error, "Settings Load");
      this.plugin.settings = Object.assign({}, DEFAULT_SETTINGS);
    }
  }
  async saveSettings() {
    try {
      await this.plugin.saveData(this.plugin.settings);
    } catch (error) {
      ErrorHandler.logError(error, "Settings Save");
      throw error;
    }
  }
  setupSettingsTab() {
    this.settingsTab = new AiVoiceMemoSettingTab(this.plugin.app, this.plugin);
    this.plugin.addSettingTab(this.settingsTab);
  }
  cleanup() {
    if (this.settingsTab) {
      this.settingsTab = null;
    }
  }
};

// src/main.ts
var AiVoiceMemoPlugin = class extends import_obsidian6.Plugin {
  async onload() {
    try {
      this.settingsManager = new SettingsManager(this);
      await this.settingsManager.loadSettings();
      this.uiManager = new UiManager(this);
      this.transcriptionService = new TranscriptionService(this);
      this.voiceRecorderManager = new VoiceRecorderManager(this, this.transcriptionService);
      this.commandManager = new CommandManager(this, this.voiceRecorderManager, this.uiManager);
      this.commandManager.registerCommands();
      this.commandManager.setupRibbonIcon();
      this.settingsManager.setupSettingsTab();
    } catch (error) {
      ErrorHandler.handleError(error);
    }
  }
  onunload() {
    try {
      this.voiceRecorderManager.cleanup();
      this.transcriptionService.cleanup();
      this.uiManager.cleanup();
      this.settingsManager.cleanup();
    } catch (error) {
      ErrorHandler.logError(error, "Plugin Unload");
    }
  }
  async saveSettings() {
    await this.settingsManager.saveSettings();
  }
  setStatusBarText(text, timeout = 0) {
    this.uiManager.setStatusBarText(text, timeout);
  }
};
